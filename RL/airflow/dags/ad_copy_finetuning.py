import logging
import os

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.utils.dates import days_ago
from config import ACCESS_KEY, CLOVA_API_KEY, SECRET_KEY
from modules import (
    CompletionExecutor,
    calculate_ad_scores,
    create_finetuning_task,
    generate_ads_comparison,
    generate_qa_data_with_comparison,
    upload_file_to_s3,
)
import pandas as pd


# âœ… ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CSV_FILE_PATH = os.path.join(BASE_DIR, "generated_ad_copies_with_likes,views,comments.csv")


SCORES_CSV = os.path.join(BASE_DIR, "generated_ad_copies_with_scores.csv")
OUTPUT_CSV_PATH = os.path.join(BASE_DIR, "hyperclovax_ab_feedback_dataset.csv")

# âœ… ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# âœ… Airflow DAG ì •ì˜
dag = DAG(
    dag_id="ad_finetuning_pipeline",
    description="ê´‘ê³  ë¬¸êµ¬ ë¹„êµ ë° Fine-Tuning DAG (ì¡°ê±´ë¶€ ì‹¤í–‰)",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
)

# âœ… íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (BashOperator)
check_file = BashOperator(
    task_id="check_csv_file",
    bash_command=f"ls -l {CSV_FILE_PATH}",
    dag=dag,
)


# âœ… 1. ì ìˆ˜ ê³„ì‚° Task
def calculate_score_task(**kwargs):
    score = calculate_ad_scores(CSV_FILE_PATH, SCORES_CSV)
    kwargs["ti"].xcom_push(key="ad_score", value=score)
    logging.info(f"ğŸ“Š ê´‘ê³  ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {score}")
    return score


calculate_score = PythonOperator(
    task_id="calculate_score",
    python_callable=calculate_score_task,
    provide_context=True,
    dag=dag,
)


# âœ… 2. ì ìˆ˜ ì¡°ê±´ í™•ì¸ Task (BranchPythonOperator)
def check_score_task(**kwargs):
    score = kwargs["ti"].xcom_pull(task_ids="calculate_score", key="ad_score")
    if score <= 9000:
        logging.info(f"ğŸ“‰ ì ìˆ˜ {score}ê°€ 9000 ì´í•˜ â†’ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        return "generate_ad_comparison"
    else:
        logging.info(f"âœ… ì ìˆ˜ {score}ê°€ 9000 ì´ˆê³¼ â†’ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì•ˆ í•¨")
        return "skip_pipeline"


check_score = BranchPythonOperator(
    task_id="check_score",
    python_callable=check_score_task,
    provide_context=True,
    dag=dag,
)


# âœ… 3. ê´‘ê³  ë¬¸êµ¬ ë¹„êµ Task
def generate_ad_comparison_task():
    logging.info(f"ğŸ“Œ ê´‘ê³  ë¬¸êµ¬ ë¹„êµ ë°ì´í„° ìƒì„± ì‹œì‘, íŒŒì¼ ê²½ë¡œ: {SCORES_CSV}")

    if not os.path.exists(SCORES_CSV):
        raise FileNotFoundError(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SCORES_CSV}")

    ads_comparison = generate_ads_comparison(SCORES_CSV)
    logging.info(f"âœ… ê´‘ê³  ë¬¸êµ¬ ë¹„êµ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(ads_comparison)}ê°œ")

    if len(ads_comparison) == 0:
        raise ValueError("âŒ ê´‘ê³  ë¬¸êµ¬ ë¹„êµ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    return ads_comparison


generate_ad_comparison = PythonOperator(
    task_id="generate_ad_comparison",
    python_callable=generate_ad_comparison_task,
    dag=dag,
)


# âœ… 4. QA ë°ì´í„° ìƒì„± Task
def generate_qa_data_task(**kwargs):
    ads_comparison = kwargs["ti"].xcom_pull(task_ids="generate_ad_comparison")

    completion_executor = CompletionExecutor(
        host="https://clovastudio.stream.ntruss.com",
        api_key=f"Bearer {CLOVA_API_KEY}",
        request_id="YOUR_REQUEST_ID",
    )

    qa_data = generate_qa_data_with_comparison(ads_comparison, completion_executor)

    qa_df = pd.DataFrame(qa_data)
    qa_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding="utf-8")
    logging.info(f"âœ… QA ë°ì´í„° CSV ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV_PATH}")

    return OUTPUT_CSV_PATH


generate_qa_data = PythonOperator(
    task_id="generate_qa_data",
    python_callable=generate_qa_data_task,
    provide_context=True,
    dag=dag,
)


# âœ… 5. S3 ì—…ë¡œë“œ Task
def upload_to_s3_task(**kwargs):
    file_path = kwargs["ti"].xcom_pull(task_ids="generate_qa_data")

    success, message = upload_file_to_s3(
        file_path,
        "testtesttesttestetst",
        "hyperclovax_ab_feedback_dataset.csv",
        ACCESS_KEY,
        SECRET_KEY,
    )

    if not success:
        raise Exception(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {message}")

    logging.info(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {message}")


upload_to_s3 = PythonOperator(
    task_id="upload_to_s3",
    python_callable=upload_to_s3_task,
    provide_context=True,
    dag=dag,
)


# âœ… 6. Fine-Tuning ìš”ì²­ Task
def finetuning_task(**kwargs):
    dataset_file = "testtesttesttestetst/hyperclovax_ab_feedback_dataset.csv"
    finetuning_response = create_finetuning_task(
        task_name="My_Ad_Finetuning_Task",
        model="HCX-003",
        tuning_type="PEFT",
        task_type="GENERATION",
        train_epochs=10,
        learning_rate=1e-5,
        dataset_file=dataset_file,
        bucket_name="testtesttesttestetst",
        access_key=ACCESS_KEY,
        secret_key=SECRET_KEY,
        api_key=CLOVA_API_KEY,
    )

    logging.info(f"âœ… Fine-Tuning ì‘ë‹µ ê²°ê³¼: {finetuning_response}")


finetuning = PythonOperator(
    task_id="finetuning",
    python_callable=finetuning_task,
    provide_context=True,
    dag=dag,
)

# âœ… 7. íŒŒì´í”„ë¼ì¸ ìŠ¤í‚µ Task
skip_pipeline = DummyOperator(
    task_id="skip_pipeline",
    dag=dag,
)

# âœ… DAG êµ¬ì¡° ì„¤ì •
check_file >> calculate_score >> check_score >> [generate_ad_comparison, skip_pipeline]
generate_ad_comparison >> generate_qa_data >> upload_to_s3 >> finetuning
